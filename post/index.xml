<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Vlad&#39;s #dev blog</title>
    <link>https://rk4n.github.io/post/</link>
    <description>Recent content in Posts on Vlad&#39;s #dev blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Aug 2016 15:59:04 +0300</lastBuildDate>
    <atom:link href="https://rk4n.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>QEMU: Post-Copy and Auto-Converge features</title>
      <link>https://rk4n.github.io/2016/08/10/qemu-post-copy-and-auto-converge-features/</link>
      <pubDate>Wed, 10 Aug 2016 15:59:04 +0300</pubDate>
      
      <guid>https://rk4n.github.io/2016/08/10/qemu-post-copy-and-auto-converge-features/</guid>
      <description>

&lt;p&gt;Cloud lifecycle is a complex process that involves several procedures such as maintenance and upgrades. These operations mostly include migration of workloads. OpenStack provides several options to perform such moving:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cold migration&lt;/li&gt;
&lt;li&gt;Live migrations&lt;/li&gt;
&lt;li&gt;Instance rebuilds&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From the list above only live migration allows moving an instance from one host to another with almost zero downtime, which is done seamlessly for the instance itself.&lt;/p&gt;

&lt;p&gt;In one of our previous researches we&amp;rsquo;ve noticed that, by default, QEMU/KVM live migration has a pretty low &lt;strong&gt;convergence&lt;/strong&gt; for heavy-loaded virtual machines. Convergence property reflects the ratio or difference between VM memory transfer speed and VM memory dirty speed during the live migration, if the memory transfer speed is higher or insignificantly lower, then live migration succeeds.&lt;/p&gt;

&lt;p&gt;In this post, we&amp;rsquo;re going to analyze different QEMU features that may help live migrations converge. The most important features are Post-Copy and Auto-Converge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Auto-Converge&lt;/strong&gt; (AC). The idea behind auto-converge is pretty simple — by throttling down the virtual CPU execution time, the guest machine is prevented from dirtying memory faster than memory can be transferred over the wire. By default, QEMU throttles vCPU by 20% and increases throttling rate by 10% each iteration. This guarantees the that guest machine eventually migrates.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Post-Copy&lt;/strong&gt; (PC). The goal of post-copy is to transfer some portion of memory in normal pre-copy mode, then switch the guest to the destination node and request any missing memory pages on demand. This comes with the additional cost of waiting for a particular page to transfer from the source node, however, post-copy guarantees that guest migrates in a constant amount of time. The downside of using post-copy is that if the destination node fails or there is a network interruption, it becomes impossible to recover the guest machine state as it is distributed between the source and destination nodes.&lt;/p&gt;

&lt;p&gt;We wanted to answer the following questions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How do various parameters of the post-copy and auto-converge features affect the migration outcome?&lt;/li&gt;
&lt;li&gt;Does the migration &lt;code&gt;max_downtime&lt;/code&gt; parameter have any impact on migration success?&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;test-harness&#34;&gt;Test Harness&lt;/h1&gt;

&lt;h2 id=&#34;test-environment&#34;&gt;Test Environment&lt;/h2&gt;

&lt;p&gt;Our setup consists of two hypervisor nodes, one monitoring VM and workstation. The monitoring VM runs on a separate machine. Tests are executed from a workstation which is connected to the same network as monitoring VM and hypervisor machines.&lt;/p&gt;

&lt;p&gt;All servers have the same configuration:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CPU&lt;/td&gt;
&lt;td&gt;1x CPU Intel Xeon E3-1230, 3.20GHz, Socket 1155, 8MB Cache, 4 core, 80W&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;RAM&lt;/td&gt;
&lt;td&gt;4x RAM 4GB Kingston KVR1333D3E9S/4GHB, DDR-III PC3-10600&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hard drive&lt;/td&gt;
&lt;td&gt;2x HDD 1.5 TB, WD Caviar Black, WD1502FAEX, SATA 6.0 Gb/s, 7200 RPM, 64MB, 3.5”&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Network&lt;/td&gt;
&lt;td&gt;1x NIC AOC-CGP-i2, PCI-E x4, 2-port Gigabit Ethernet LAN&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Software versions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Operating system: Ubuntu 16.04 LTS&lt;/li&gt;
&lt;li&gt;QEMU version: 2.6.0&lt;/li&gt;
&lt;li&gt;Libvirt version: 1.3.5&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tools&#34;&gt;Tools&lt;/h2&gt;

&lt;p&gt;The test suite consists of 3 main parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Orchestration - &lt;a href=&#34;https://github.com/timofei-durakov/orchestra&#34;&gt;Orchestra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Metrics storage - &lt;a href=&#34;https://www.influxdata.com/time-series-platform/influxdb/&#34;&gt;Influxdb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Monitoring agents, - &lt;a href=&#34;https://www.influxdata.com/time-series-platform/telegraf/&#34;&gt;Telegraf&lt;/a&gt; and &lt;a href=&#34;https://github.com/rk4n/migration-monitor&#34;&gt;migration monitor&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://people.seas.harvard.edu/~apw/stress/&#34;&gt;Stress&lt;/a&gt; tool for dirtying the VM memory (example of cmdline parameters: &lt;code&gt;--vm 1 --vm-bytes 128M&lt;/code&gt;. Number of workers and amount of memory per each).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Orchestra is responsible for managing instances during scenario run (boot, live-migrate, etc.), and also enabling/disabling monitoring. Ansible playbooks are used for update-config/setup up monitoring on hypervisor nodes or booted instances.&lt;/p&gt;

&lt;p&gt;Influxdb is a time series database that is used for storing events from host hypervisors/instances.
Migration monitor - a special tool that allows capturing events from libvirt to measure live-migration time, timeouts, dirty pages rate.&lt;/p&gt;

&lt;h2 id=&#34;test-scenarios&#34;&gt;Test scenarios&lt;/h2&gt;

&lt;p&gt;The first set of scenarios is aimed to explore how different settings of auto-converge parameters affect the migration duration and outcome. For all tests, we’ve used guest machines with 1 vCPU and 2Gb of RAM. As a baseline for the load we’ve used 2 memory workers dirtying constantly 512Mb each, such load is sufficient to make default live migration last forever.&lt;/p&gt;

&lt;h3 id=&#34;parameters-definitions&#34;&gt;Parameters definitions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--auto-converge&lt;/code&gt;: Enables auto-converge feature. The initial guest CPU throttling rate can be set with &lt;code&gt;auto-converge-initial&lt;/code&gt;(or &lt;code&gt;x-cpu-throttle-initial&lt;/code&gt;). If the initial throttling rate is not enough to ensure convergence, the rate is periodically increased by &lt;code&gt;auto-converge-increment&lt;/code&gt;(&lt;code&gt;x-cpu-throttle-increment&lt;/code&gt;).&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;x-cpu-throttle-initial&lt;/code&gt; and &lt;code&gt;x-cpu-throttle-increment&lt;/code&gt;: QEMU migration parameters specified via monitor command: &lt;code&gt;virsh qemu-monitor-command &amp;lt;domain_name&amp;gt; --hmp --cmd &amp;quot;migration_set_parameter x-cpu-throttle-initial 50&amp;quot;&lt;/code&gt;.
According to &lt;a href=&#34;http://wiki.qemu.org/Features/AutoconvergeLiveMigration&#34;&gt;Features/AutoconvergeLiveMigration&lt;/a&gt;, the initial throttling percentage defaults to 20%. If after a period of time the migration has still not completed then throttling is incremented. This process continues until migration completes or we reach 99% throttled. By default the throttling rate is always incremented by 10%.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--postcopy&lt;/code&gt;: enables post-copy logic in migration, but does not actually start post-copy.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--postcopy-after-precopy&lt;/code&gt;: let libvirt automatically switch to post-copy after the first pass of pre-copy is finished.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--compressed&lt;/code&gt;: activates compression, the compression method is chosen with &amp;ndash;comp-methods. Supported methods are &amp;ldquo;mt&amp;rdquo; and &amp;ldquo;xbzrle&amp;rdquo; and can be used in any combination. When no methods are specified, a hypervisor default methods will be used. QEMU defaults to &amp;ldquo;xbzrle&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--timeout-postcopy&lt;/code&gt;: When &amp;ndash;timeout-postcopy is used, virsh will switch migration from pre-copy to post-copy upon timeout.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;migrate-setmaxdowntime&lt;/code&gt;: Set maximum tolerable downtime for a domain which is being live-migrated to another host.  The downtime is a number of milliseconds the guest is allowed to be down at the end of live migration.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;#&lt;/th&gt;
&lt;th&gt;Load&lt;/th&gt;
&lt;th&gt;Auto-Converge params&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;1.&lt;/td&gt;
&lt;td&gt;2 workers, &lt;br/&gt; 512Mb mem allocated&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--auto-converge&lt;/code&gt; &lt;br/&gt; &lt;code&gt;x-cpu-throttle-initial 20&lt;/code&gt; &lt;br/&gt; &lt;code&gt;x-cpu-throttle-increment 10&lt;/code&gt;&lt;br/&gt;&lt;strong&gt;default settings&lt;/strong&gt;, see &lt;a href=&#34;https://github.com/qemu/qemu/blob/507e4ddc3abf67391bcbc9624fd60b969c159b78/migration/migration.c#L55-L56&#34;&gt;migration.c&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2.&lt;/td&gt;
&lt;td&gt;&lt;em&gt;same&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--auto-converge&lt;/code&gt; &lt;br/&gt; &lt;code&gt;x-cpu-throttle-initial 30&lt;/code&gt; &lt;br/&gt; &lt;code&gt;x-cpu-throttle-increment 15&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;3.&lt;/td&gt;
&lt;td&gt;&lt;em&gt;same&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--auto-converge&lt;/code&gt; &lt;br/&gt; &lt;code&gt;x-cpu-throttle-initial 50&lt;/code&gt; &lt;br/&gt; &lt;code&gt;x-cpu-throttle-increment 20&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Next part is to investigate post-copy related parameters.
The difference between scenarios 4 and 5 is that by default QEMU uses pre-copy, and post-copy should be either triggered explicitly during the migration or internally by timeout. Flag &lt;code&gt;--postcopy-after-precopy&lt;/code&gt; indicates that post-copy step will be triggered after the first iteration of pre-copy. Flag &lt;code&gt;--timeout-postcopy&lt;/code&gt; indicates that post-copy step will be automatically triggered after a timeout.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;#&lt;/th&gt;
&lt;th&gt;Load&lt;/th&gt;
&lt;th&gt;Post-Copy params&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;4.&lt;/td&gt;
&lt;td&gt;2 workers, &lt;br/&gt; 512Mb mem allocated&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--auto-converge&lt;/code&gt; &lt;br/&gt; &lt;code&gt;--timeout-postcopy&lt;/code&gt; &lt;br/&gt; &lt;code&gt;--timeout 60&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;5.&lt;/td&gt;
&lt;td&gt;&lt;em&gt;same&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--postcopy&lt;/code&gt; &lt;br/&gt; &lt;code&gt;--postcopy-after-precopy&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;6.&lt;/td&gt;
&lt;td&gt;&lt;em&gt;same&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--postcopy&lt;/code&gt; &lt;br/&gt; &lt;code&gt;--postcopy-after-precopy&lt;/code&gt; &lt;br/&gt; &lt;code&gt;--compressed&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Next two scenarios define more aggressive load and aim to test both auto-converge and post-copy migrations under that load pattern.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;#&lt;/th&gt;
&lt;th&gt;Load&lt;/th&gt;
&lt;th&gt;Auto-Converge/Post-Copy params&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;7.&lt;/td&gt;
&lt;td&gt;3 workers, &lt;br/&gt; 512Mb mem allocated&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--auto-converge&lt;/code&gt; &lt;br/&gt; &lt;code&gt;x-cpu-throttle-initial 50&lt;/code&gt; &lt;br/&gt; &lt;code&gt;x-cpu-throttle-increment 20&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;8.&lt;/td&gt;
&lt;td&gt;&lt;em&gt;same&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--postcopy&lt;/code&gt; &lt;br/&gt; &lt;code&gt;--postcopy-after-precopy&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;Finally, we’re going to test how migration &lt;code&gt;max_downtime&lt;/code&gt; parameter affects the live migration duration.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;#&lt;/th&gt;
&lt;th&gt;Load&lt;/th&gt;
&lt;th&gt;Post-Copy params&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;9.&lt;/td&gt;
&lt;td&gt;3 workers, &lt;br/&gt; 512Mb mem allocated&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--auto-converge&lt;/code&gt; &lt;br/&gt; default settings&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;10.&lt;/td&gt;
&lt;td&gt;&lt;em&gt;same&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--auto-converge&lt;/code&gt; &lt;br/&gt; &lt;code&gt;migrate-setmaxdowntime 1000&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;11.&lt;/td&gt;
&lt;td&gt;&lt;em&gt;same&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--auto-converge&lt;/code&gt; &lt;br/&gt; &lt;code&gt;migrate-setmaxdowntime 5000&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h1 id=&#34;test-results&#34;&gt;Test Results&lt;/h1&gt;

&lt;p&gt;Graphical examples of the Live migration with default auto-converge, customized auto-converge and default post-copy modes. Each figure has three graphs: migration duration, mem dirtying rate, and remaining memory dynamics. Each figure shows 5 min timeframe to make it easier to reason about migration duration. Load pattern is the same: 2 workers with 512Mb allocated for dirtying.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rk4n.github.io/img/qemu-lv-1/1.png&#34; alt=&#34;Figure 1&#34; /&gt;
&lt;em&gt;Figure 1 — default auto-converge&lt;/em&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rk4n.github.io/img/qemu-lv-1/2.png&#34; alt=&#34;Figure 2&#34; /&gt;
&lt;em&gt;Figure 2 — customized auto-converge (initial: 30%, increment: 15%)&lt;/em&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rk4n.github.io/img/qemu-lv-1/3.png&#34; alt=&#34;Figure 3&#34; /&gt;
&lt;em&gt;Figure 3 — post-copy&lt;/em&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;Test results for the first two sets of scenarios (auto-converge and post-copy with baseline load config):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rk4n.github.io/img/qemu-lv-1/0_chart_0.png&#34; alt=&#34;Figure 4&#34; /&gt;
&lt;em&gt;Figure 4 — auto-converge and post-copy comparison chart&lt;/em&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;First three scenarios look pretty reasonable: QEMU with default settings for auto-converge migrates machine in 252 seconds, with more tough custom parameters migration time decreases. Scenario #4 showed slightly better results rather than #1 which requires some additional investigation, as QEMU should turn on post-copy (which we see in #5 and #6 is much faster compared to auto-converge) after 60 seconds.
&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;Success rate for all scenarios except #6 is 100% :&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Scenario&lt;/th&gt;
&lt;th&gt;Success Rate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1. default&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2. AC custom&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3. AC custom tough&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4. PC after 60s&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5. default PC&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6. PC with xbzrle&lt;/td&gt;
&lt;td&gt;40%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With increased load (3 workers, 512Mb allocated) situation is pretty the same for auto-converge and post-copy: auto-converge is much slower than post-copy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rk4n.github.io/img/qemu-lv-1/0_chart_1.png&#34; alt=&#34;Figure 5&#34; /&gt;
&lt;em&gt;Figure 5 — auto-converge and post-copy comparison chart for increased load&lt;/em&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;The final part is the comparison of different settings for the &lt;code&gt;max_downtime&lt;/code&gt; parameter. Greater &lt;code&gt;max_downtime&lt;/code&gt; you have, less time to migrate you need.&lt;/p&gt;

&lt;p&gt;Each figure shows &lt;strong&gt;7 min&lt;/strong&gt; timeframe. Default &lt;code&gt;max_downtime&lt;/code&gt; value for QEMU is 300(ms), according to QEMU source code (2.6.0 release branch).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rk4n.github.io/img/qemu-lv-1/4.png&#34; alt=&#34;Figure 6&#34; /&gt;
&lt;em&gt;Figure 6 — Auto-converge with default &lt;code&gt;max_downtime&lt;/code&gt; settings&lt;/em&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rk4n.github.io/img/qemu-lv-1/5.png&#34; alt=&#34;Figure 7&#34; /&gt;
&lt;em&gt;Figure 7 — Auto-converge with &lt;code&gt;max_downtime&lt;/code&gt; set to 1000 (ms)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This is a less realistic scenario (&lt;code&gt;max_downtime = 5000ms&lt;/code&gt;), it is shown just to compare it with 1000ms and 300ms. A better option is to use OpenStack Nova &lt;code&gt;force_complete&lt;/code&gt; feature that allows heavy loaded machines that cannot be live-migrated to be paused during the migration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rk4n.github.io/img/qemu-lv-1/6.png&#34; alt=&#34;Figure 8&#34; /&gt;
&lt;em&gt;Figure 8 — Auto-converge with &lt;code&gt;max_downtime&lt;/code&gt; set to 5000 (ms)&lt;/em&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;And summary chart looks like this:
&lt;img src=&#34;https://rk4n.github.io/img/qemu-lv-1/0_chart_2.png&#34; alt=&#34;Figure 9&#34; /&gt;
&lt;em&gt;Figure 9 — auto-converge with different &lt;code&gt;max_downtime&lt;/code&gt; settings&lt;/em&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;Our aim was a comparison of different live migration techniques under various configuration parameters, in order to demonstrate and recommend an appropriate usage of post-copy and auto-converge.&lt;/p&gt;

&lt;p&gt;From the results above, we see that post-copy performs much better than other techniques, except when compression is used simultaneously. It&amp;rsquo;s actually not quite true, because during the post-copy phase VM performance degrades significantly, which was not measured.&lt;/p&gt;

&lt;p&gt;Both, auto-converge and post-copy, perform very well. Live migration in a post-copy mode can finish in a finite time. However, memory of a VM is spread between source and destination node, so in case of any failure there is a risk that the VM will need to be rebooted on destination. There is no such risk when using auto-converge, but it does not guarantee that live migration will end. Something that still needs a research is how post-copy impacts workload running on a VM and a comparison of that with how auto-converge impacts the workload.&lt;/p&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;p&gt;The following materials were used:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;“Dive Into VM Live Migration” &lt;a href=&#34;https://01.org/sites/default/files/dive_into_vm_live_migration_2.pdf&#34;&gt;https://01.org/sites/default/files/dive_into_vm_live_migration_2.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;“Analysis of techniques for ensuring migration completion with KVM” &lt;a href=&#34;https://www.berrange.com/posts/2016/05/12/analysis-of-techniques-for-ensuring-migration-completion-with-kvm/&#34;&gt;https://www.berrange.com/posts/2016/05/12/analysis-of-techniques-for-ensuring-migration-completion-with-kvm/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>